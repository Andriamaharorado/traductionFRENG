{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nom : Andriamaharo\n",
    "- Prénoms : Kwon-Chui Rado Angelin\n",
    "- Examen : RNN\n",
    "- Projet : Traduction de Texte Francais-Anglais\n",
    "- Niveau : M1-I2AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lancer sur GoogleCoolab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Installation du pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.0.0 torchtext==0.15.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Charger le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "# Initialisation des listes\n",
    "train_english_sentences = []\n",
    "train_french_sentences = []\n",
    "\n",
    "# Fonction pour nettoyer les caractères Unicode spéciaux et la ponctuation\n",
    "def clean_text(text):\n",
    "    # Normalisation Unicode pour uniformiser les caractères spéciaux\n",
    "    return unicodedata.normalize(\"NFKC\", text).replace(\"\\u202f\", \" \").strip()\n",
    "\n",
    "# Fonction pour retirer la ponctuation (utilisée pendant l'entraînement)\n",
    "def remove_punctuation(sentence):\n",
    "    return sentence.strip(string.punctuation).strip()\n",
    "\n",
    "# Lecture et traitement du fichier\n",
    "with open(\"/content/fra.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        # Séparation des éléments par tabulation\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) >= 2:  # S'assurer qu'il y a au moins deux colonnes\n",
    "            # Nettoyage de base\n",
    "            english = clean_text(parts[0])\n",
    "            french = clean_text(parts[1])\n",
    "\n",
    "            # Préparer les données d'entraînement sans ponctuation\n",
    "            train_english_sentences.append(remove_punctuation(english))\n",
    "            train_french_sentences.append(remove_punctuation(french))\n",
    "\n",
    "# Affichage des résultats pour l'entraînement\n",
    "print(\"Training English Sentences:\", train_english_sentences[:10])  # Exemple des 10 premières phrases\n",
    "print(\"Training French Sentences:\", train_french_sentences[:10])   # Exemple des 10 premières phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "# Tokenisation\n",
    "def tokenize(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "# Construire les vocabulaires\n",
    "def build_vocab(sentences):\n",
    "    vocab = {word: i + 3 for i, word in enumerate(set(\" \".join(sentences).split()))}  # +3 pour les tokens spéciaux\n",
    "    vocab[\"<pad>\"] = 0\n",
    "    vocab[\"<start>\"] = 1\n",
    "    vocab[\"<end>\"] = 2\n",
    "    return vocab\n",
    "\n",
    "french_vocab = build_vocab(train_french_sentences[:50000])\n",
    "english_vocab = build_vocab(train_english_sentences[:50000])\n",
    "inv_english_vocab = {v: k for k, v in english_vocab.items()}\n",
    "\n",
    "# Encodage des phrases\n",
    "def encode_sentence(sentence, vocab):\n",
    "    tokens = [\"<start>\"] + tokenize(sentence) + [\"<end>\"]\n",
    "    return [vocab.get(token, vocab[\"<pad>\"]) for token in tokens]\n",
    "\n",
    "encoded_french = [encode_sentence(s, french_vocab) for s in train_french_sentences[:50000]]\n",
    "encoded_english = [encode_sentence(s, english_vocab) for s in train_english_sentences[:50000]]\n",
    "\n",
    "# Dataset personnalisé\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_data, target_data):\n",
    "        self.source_data = source_data\n",
    "        self.target_data = target_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.source_data[idx]), torch.tensor(self.target_data[idx])\n",
    "\n",
    "dataset = TranslationDataset(encoded_french, encoded_english)\n",
    "\n",
    "# Collate function pour gérer le padding\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Modèle Seq2Seq avec LSTM\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, hidden_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.embedding_enc = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.embedding_dec = nn.Embedding(output_vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        # Encoder\n",
    "        embedded_src = self.embedding_enc(src)\n",
    "        _, (hidden, cell) = self.encoder(embedded_src)\n",
    "\n",
    "        # Decoder\n",
    "        batch_size = tgt.size(0)\n",
    "        seq_len = tgt.size(1)\n",
    "        outputs = torch.zeros(batch_size, seq_len, len(english_vocab)).to(src.device)\n",
    "\n",
    "        input_token = tgt[:, 0].unsqueeze(1)  # Premier token du décodeur (<start>)\n",
    "        for t in range(1, seq_len):\n",
    "            embedded_dec = self.embedding_dec(input_token)\n",
    "            output, (hidden, cell) = self.decoder(embedded_dec, (hidden, cell))\n",
    "            logits = self.fc(output)\n",
    "\n",
    "            outputs[:, t, :] = logits.squeeze(1)\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = logits.argmax(2)\n",
    "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Initialisation du modèle\n",
    "input_vocab_size = len(french_vocab)\n",
    "output_vocab_size = len(english_vocab)\n",
    "embedding_dim = 16\n",
    "hidden_size = 32\n",
    "model = Seq2Seq(input_vocab_size, output_vocab_size, embedding_dim, hidden_size)\n",
    "\n",
    "# Optimiseur et fonction de perte\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Entraînement\n",
    "epochs = 50\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        teacher_forcing_ratio = max(0.5 - (epoch / epochs) * 0.5, 0.1)\n",
    "        for src, tgt in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt, teacher_forcing_ratio)\n",
    "\n",
    "            # Supprimer le premier token (<start>) pour la perte\n",
    "            output = output[:, 1:, :].reshape(-1, output_vocab_size)\n",
    "            target = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}, Teacher Forcing Ratio: {teacher_forcing_ratio:.2f}\")\n",
    "\n",
    "# Traduction\n",
    "def translate(sentence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = torch.tensor(encode_sentence(sentence, french_vocab)).unsqueeze(0)\n",
    "        embedded_src = model.embedding_enc(src)\n",
    "        _, (hidden, cell) = model.encoder(embedded_src)\n",
    "\n",
    "        outputs = []\n",
    "        input_token = torch.tensor([[english_vocab[\"<start>\"]]]).to(src.device)\n",
    "        for _ in range(20):  # Longueur maximale pour la sortie\n",
    "            embedded_dec = model.embedding_dec(input_token)\n",
    "            output, (hidden, cell) = model.decoder(embedded_dec, (hidden, cell))\n",
    "            logits = model.fc(output)\n",
    "            top1 = logits.argmax(2).item()\n",
    "            if top1 == english_vocab[\"<end>\"]:\n",
    "                break\n",
    "            outputs.append(top1)\n",
    "            input_token = torch.tensor([[top1]]).to(src.device)\n",
    "        return \" \".join([inv_english_vocab[idx] for idx in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Faire le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la traduction\n",
    "value=\"bonjour, je suis libre\"\n",
    "print(\"la traduction de \", value, \"est de :\", translate(value))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
